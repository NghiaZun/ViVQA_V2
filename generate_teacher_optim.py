"""
teacher_generate.py ‚Äì STABLE version v·ªõi t·ªëi ∆∞u ƒë∆°n gi·∫£n
Author: Nghia-Duong (stable + faster)
"""

import os
import json
import re
import pandas as pd
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForVision2Seq

# ===========================
# CONFIG
# ===========================
CSV_PATH = "/kaggle/input/vivqa/ViVQA-main/ViVQA-main/train.csv"  # GT-guided dataset
IMAGE_DIR = "/kaggle/input/vivqa/drive-download-20220309T020508Z-001/train"
MODEL_NAME = "Qwen/Qwen2-VL-7B-Instruct"
OUT_JSONL = "/kaggle/working/teacher_outputs_gt_guided.jsonl"

# Reasoning type keywords for auto-classification
REASONING_KEYWORDS = {
    "COUNTING": ["bao nhi√™u", "m·∫•y", "s·ªë l∆∞·ª£ng", "ƒë·∫øm"],
    "SPATIAL": ["·ªü ƒë√¢u", "v·ªã tr√≠", "ph√≠a", "tr√™n", "d∆∞·ªõi", "trong", "ngo√†i"],
    "CAUSAL": ["t·∫°i sao", "v√¨ sao", "l√Ω do", "nguy√™n nh√¢n"],
    "OBJECT": ["c√°i g√¨", "con g√¨", "l√† g√¨", "v·∫≠t g√¨"],
    "INTENT": ["m·ª•c ƒë√≠ch", "√Ω ƒë·ªãnh", "d√πng ƒë·ªÉ"],
    "COMMONSENSE": ["n√™n", "th∆∞·ªùng", "c√≥ th·ªÉ", "ph·∫£i"],
    "DESCRIPTIVE": []
}

REASONING_WEIGHTS = {
    "CAUSAL": 5.0,
    "DESCRIPTIVE": 4.0,
    "INTENT": 4.0,
    "OBJECT": 2.0,
    "COUNTING": 2.0,
    "SPATIAL": 1.5,
    "COMMONSENSE": 1.0
}

def infer_reasoning_type(question: str) -> str:
    """Auto-classify reasoning type from question"""
    q_lower = question.lower().strip()
    for rtype, keywords in REASONING_KEYWORDS.items():
        if rtype == "DESCRIPTIVE":
            continue
        for kw in keywords:
            if kw in q_lower:
                return rtype
    return "DESCRIPTIVE"

# ===========================
# LOAD MODEL - ƒê∆†N GI·∫¢N H√ìA
# ===========================
device = "cuda:0"  # Ch·ªâ d√πng GPU ƒë·∫ßu ti√™n cho ·ªïn ƒë·ªãnh
print(f"[INFO] Using device: {device}")

processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",  # ƒê·ªÉ t·ª± ƒë·ªông ch·ªçn, nh∆∞ng s·∫Ω ∆∞u ti√™n GPU 0
    trust_remote_code=True,
    low_cpu_mem_usage=True
)
model.eval()

# ===========================
# PARSE OUTPUT - SIMPLE FORMAT
# ===========================
def parse_structured_output(text: str, question: str = ""):
    """Parse simple format: Answer: X / Type: Y / Reasoning: Z v·ªõi validation"""
    answer, reasoning, reasoning_type = "", "", ""
    lines = text.strip().split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('Answer:'):
            answer = line.split(':', 1)[1].strip()
        elif line.startswith('Type:'):
            reasoning_type = line.split(':', 1)[1].strip().upper()
            # Clean type: ch·ªâ l·∫•y keyword ƒë·∫ßu ti√™n
            reasoning_type = reasoning_type.split()[0] if reasoning_type else ""
        elif line.startswith('Reasoning:'):
            reasoning = line.split(':', 1)[1].strip()
    
    # VALIDATION: L·ªçc reasoning x·∫•u
    bad_patterns = [
        "(1 c√¢u gi·∫£i th√≠ch)", "(1 c√¢u)", "...", 
        "Ch·ªçn Type", "Format:", "B·∫ÆT BU·ªòC"
    ]
    if reasoning and any(bad in reasoning for bad in bad_patterns):
        reasoning = ""  # Mark as invalid
    
    # Fallback to heuristic if no type found
    if not reasoning_type and question:
        reasoning_type = infer_reasoning_type(question)
    
    # Validate type
    valid_types = ["COUNTING", "SPATIAL", "CAUSAL", "OBJECT", "DESCRIPTIVE", "COMMONSENSE", "INTENT"]
    if reasoning_type not in valid_types:
        reasoning_type = infer_reasoning_type(question)
    
    return answer, reasoning, reasoning_type

# ===========================
# TEACHER GENERATION - GT-GUIDED + OPTIMIZED
# ===========================
@torch.no_grad()
def call_teacher_qwen(image_path: str, question: str, ground_truth: str):
    """GT-guided: Teacher explains WHY answer is ground_truth"""
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        return None

    # IMPROVED: Cleaner prompt v·ªõi examples ƒë·ªÉ model follow t·ªët h∆°n
    user_prompt = f"""D·ª±a v√†o h√¨nh ·∫£nh, tr·∫£ l·ªùi c√¢u h·ªèi v·ªõi format ch√≠nh x√°c:

C√¢u h·ªèi: {question}
ƒê√°p √°n ƒë√∫ng: {ground_truth}

Vi·∫øt gi·∫£i th√≠ch T·∫†I SAO ƒë√°p √°n l√† "{ground_truth}".

B·∫ÆT BU·ªòC format (3 d√≤ng):
Answer: {ground_truth}
Type: [COUNTING ho·∫∑c SPATIAL ho·∫∑c CAUSAL ho·∫∑c OBJECT ho·∫∑c DESCRIPTIVE ho·∫∑c COMMONSENSE ho·∫∑c INTENT]
Reasoning: [Gi·∫£i th√≠ch d·ª±a v√†o h√¨nh ·∫£nh, 1 c√¢u ho√†n ch·ªânh]

V√≠ d·ª•:
Answer: m√†u xanh l√°
Type: DESCRIPTIVE
Reasoning: H√¨nh ·∫£nh cho th·∫•y chi·∫øc xe bu√Ωt c√≥ m√†u xanh l√°.

B√¢y gi·ªù tr·∫£ l·ªùi:"""

    enhanced_system_prompt = "B·∫°n l√† tr·ª£ l√Ω VQA chuy√™n nghi·ªáp. Lu√¥n tu√¢n th·ªß format 3 d√≤ng: Answer, Type, Reasoning."

    messages = [
        {"role": "system", "content": enhanced_system_prompt},
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": user_prompt}
        ]}
    ]

    try:
        text_prompt = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = processor(
            text=[text_prompt],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(device)

        # Mixed precision + optimized generation
        with torch.amp.autocast('cuda'):
            output = model.generate(
                **inputs,
                max_new_tokens=100,       # TƒÉng l√™n ƒë·ªÉ ƒë·ªß ch·ªó cho reasoning ƒë·∫ßy ƒë·ªß
                min_new_tokens=30,        # ƒê·∫£m b·∫£o sinh ƒë·ªß 3 d√≤ng
                do_sample=False,          # Greedy = faster + deterministic
                temperature=1.0,
                use_cache=True,
                repetition_penalty=1.1,   # Tr√°nh l·∫∑p l·∫°i
                pad_token_id=processor.tokenizer.pad_token_id
            )

        gen = processor.batch_decode(
            output[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )[0].strip()

        answer, reasoning, reasoning_type = parse_structured_output(gen, question)

        # QUALITY CHECK: ƒê·∫£m b·∫£o reasoning h·ª£p l·ªá
        if not reasoning or len(reasoning) < 10:
            # Retry v·ªõi prompt ƒë∆°n gi·∫£n h∆°n n·∫øu l·∫ßn ƒë·∫ßu fail
            return None
        
        # Clean raw output: lo·∫°i b·ªè ph·∫ßn h∆∞·ªõng d·∫´n th·ª´a
        clean_raw = "\n".join([
            f"Answer: {answer}",
            f"Type: {reasoning_type}",
            f"Reasoning: {reasoning}"
        ])

        return {
            "answer": answer,
            "reasoning": reasoning,
            "reasoning_type": reasoning_type,
            "raw": clean_raw,  # L∆∞u clean version thay v√¨ raw
            "reasoning_weight": REASONING_WEIGHTS.get(reasoning_type, 1.0)
        }

    except Exception as e:
        print(f"[ERROR] Generation failed for {image_path}: {e}")
        return None

# ===========================
# MAIN LOOP - C·∫¢I THI·ªÜN
# ===========================
df = pd.read_csv(CSV_PATH)
results = []

# RESUME t·ª´ checkpoint n·∫øu c√≥
processed_ids = set()
if os.path.exists(OUT_JSONL):
    print(f"[INFO] üîÑ Found existing checkpoint: {OUT_JSONL}")
    with open(OUT_JSONL, "r", encoding="utf-8") as f:
        for line in f:
            try:
                r = json.loads(line)
                results.append(r)
                processed_ids.add(r["img_id"])
            except:
                continue
    print(f"[INFO] ‚úÖ Resumed with {len(results)} existing samples")

# Periodic save ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
SAVE_INTERVAL = 50  # Save th∆∞·ªùng xuy√™n h∆°n (m·ªói 50 samples)
failed_samples = 0  # Track failed generations

print(f"[INFO] Total samples to process: {len(df)} | Already done: {len(processed_ids)}")
print(f"[INFO] Quality filters enabled: reasoning validation + format check")

try:
    for idx, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc="GT-Guided Teacher")):
        image_id = str(row.get("img_id", row.get("image_id", ""))).strip()
        
        # SKIP n·∫øu ƒë√£ x·ª≠ l√Ω r·ªìi
        if image_id in processed_ids:
            continue
        
        image_path = os.path.join(IMAGE_DIR, f"{image_id}.jpg")
        
        if not os.path.exists(image_path):
            continue

        q = str(row["question"]).strip()
        gt_answer = str(row["answer"]).strip()  # Ground truth

        res = call_teacher_qwen(image_path, q, gt_answer)

        if res and res["answer"] and res["reasoning"]:  # STRICTER: Ph·∫£i c√≥ c·∫£ answer V√Ä reasoning
            new_entry = {
                "img_id": image_id,
                "image_path": image_path,
                "question": q,
                "reasoning_type": res["reasoning_type"],
                "teacher_answer": res["answer"],
                "teacher_reasoning": res["reasoning"],
                "teacher_raw": res["raw"],
                "reasoning_weight": res["reasoning_weight"]
            }
            results.append(new_entry)
            processed_ids.add(image_id)
            
            # APPEND mode: Save ngay l·∫≠p t·ª©c sau m·ªói sample th√†nh c√¥ng
            with open(OUT_JSONL, "a", encoding="utf-8") as f:
                f.write(json.dumps(new_entry, ensure_ascii=False) + "\n")
        else:
            failed_samples += 1
            if failed_samples <= 5:  # Log first 5 failures
                print(f"\n[SKIP] Failed sample: {image_id} | Q: {q[:40]}...")
        
        # Progress report ƒë·ªãnh k·ª≥
        if len(results) % SAVE_INTERVAL == 0 and len(results) > 0:
            print(f"\n[INFO] üíæ Progress: {len(results)} samples saved | Failed: {failed_samples}")
        
        # Memory management m·ªói 100 samples
        if idx % 100 == 0:
            torch.cuda.empty_cache()
            import gc
            gc.collect()  # Python garbage collection

except KeyboardInterrupt:
    print(f"\n[WARN] ‚ö†Ô∏è Interrupted by user! Saving progress...")
    print(f"[INFO] Saved {len(results)} samples before interruption")
finally:
    # Final report (file ƒë√£ ƒë∆∞·ª£c save li√™n t·ª•c r·ªìi, kh√¥ng c·∫ßn save l·∫°i)
    print(f"\n[INFO] ‚úÖ Completed! Total saved: {len(results)}/{len(df)} teacher samples ‚Üí {OUT_JSONL}")
    if len(results) > 0:
        print(f"[INFO] Success rate: {len(results)/len(df)*100:.1f}% | Failed: {failed_samples}")
        print(f"[INFO] Average reasoning length: {sum(len(r['teacher_reasoning']) for r in results)/len(results):.1f} chars")
    else:
        print(f"[WARN] No valid samples generated!")
